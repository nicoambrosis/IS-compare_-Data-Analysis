{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Este programa permite limpiar las columnas 'REF_Flank1.locus_tag' y 'REF_Flank2.locus_tag' de forma tal de que en cada celda solo se informe UN locus bajo el formato BP1234. Las funciones definidas en este bloques de codigo han sido pensadas para modificar las celdas dando como resultado el locus que se encuentra mas cerca de la IS.\n",
    "\n",
    "Las diferentes formas en que se presentaban los datos en las celdas de las columnas 'REF_Flank1.locus_tag' y 'REF_Flank2.locus_tag' han sido caracterizadas con nombres que van desde type_1 a type_6 para 'REF_Flank1.locus_tag' y type_1 a type_8 para 'REF_Flank2.locus_tag'. Mas abajo se describe el tipo de dato para cada clasificacion.\n",
    "\n",
    "\n",
    "Luego se adicionarion lineas de codigo para combinar las columnas 'REF_Flank1.locus_tag' y 'REF_Flank2.locus_tag' de forma tal de conseguir UNA columna que contenga ambos datos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 45)\n",
    "pd.set_option('display.max_rows', 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1 Data Cleanning columna 'REF_Flank1.locus_tag' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('ISs_Strains.csv')\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    print(f'\\n{i}')\n",
    "\n",
    "\"\"\"\n",
    "Los resultados que siguen a continuacion son casos especiales que deberiamos limpiar.\n",
    "IMPORTANTE: notar que cada tipo de dato que queremos limpiar tiene una len() particular, por\n",
    "lo tanto podemos elegir ese criterio para definir las funciones que vamos a usar para limpiar cada tipo de dato.\n",
    "\n",
    "type_1: 'BP1047 | BP1048' \n",
    "\n",
    "type_2: '-', '- | -', '- | - | -'\n",
    "\n",
    "type_3: 'BP0123 | BP0124 | -'\n",
    "\n",
    "type_4: 'BP3053 | BP3054 | BP3055'\n",
    "\n",
    "type_5: 'BP3508 | -' o 'BP0692 | - | -'\n",
    "\n",
    "type_6: 'BP2524 | BP2524A'  finalemente decidi que iba a dejar la letra A porque asi esta anotado en Tohama\n",
    "\n",
    "type_7: 'BP0101B' finalmente no lo considere como un tipo de dato a limpiar porque en el genoma de Tohama esta anotado asi\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_1: ej 'BP1047 | BP1048'\n",
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 15:\n",
    "        print(i)\n",
    "    else: pass\n",
    "\n",
    "# x = 'BP1047 | BP1048'\n",
    "# x.split(' ')[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_2: '-', '- | -', '- | - | -'\n",
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if i == '-' or i == '- | -' or i == '- | - | -':\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_3: ej 'BP0123 | BP0124 | -'\n",
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 19:\n",
    "        print(i)\n",
    "    else: pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_4: ej 'BP3053 | BP3054 | BP3055'\n",
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 24:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # type_5: ej 'BP3508 | -' or 'BP0692 | - | -'\n",
    "\n",
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 10 or len(i) == 14:\n",
    "        print(i)\n",
    "    else: pass\n",
    "\n",
    "# len('BP0692 | - | -')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_6: ej 'BP2524 | BP2524A'\n",
    "\n",
    "df = pd.read_csv('ISs_Strains.csv')\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 16:\n",
    "        print(i)\n",
    "        \n",
    "    else: pass\n",
    "    \n",
    "# x = 'BP2029 | BP2029A'    \n",
    "\n",
    "# x.split(' ')[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Cleanning Functions-Flank1-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "\n",
    "# type_1: ej 'BP1047 | BP1048'>>> for 7<x<16: split(' ') y quedarse con el el elemento [2] \n",
    "def clean_type_1(x):    \n",
    "    if len(x) == 15:\n",
    "        return x.split(' ')[2]\n",
    "    else: \n",
    "        return x\n",
    "\n",
    "#######################################################\n",
    "def clean_type_2(x):\n",
    "    if x == '-' or x == '- | -' or x == '- | - | -':\n",
    "        return 'NaN'\n",
    "        \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "#######################################################\n",
    "# type_3: ej 'BP0123 | BP0124 | -'\n",
    "def clean_type_3(x):\n",
    "    if len(x) == 19:\n",
    "        return x.split(' ')[2]\n",
    "    else: return x\n",
    "        \n",
    "########################################################\n",
    "# type_4: ej 'BP3053 | BP3054 | BP3055'\n",
    "def clean_type_4(x):\n",
    "    if len(x) == 24:\n",
    "        return x.split(' ')[4]\n",
    "    else: return x\n",
    "\n",
    "#######################################################\n",
    "# type_5: ej 'BP3508 | -'\n",
    "def clean_type_5(x):\n",
    "    if len(x) == 10 or len(x) == 14:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "\n",
    "#########################################################\n",
    "# type_6: ej 'BP2524 | BP2524A'\n",
    "def clean_type_6(x):\n",
    "    if len(x) == 16:\n",
    "        return x.split(' ')[2]\n",
    "    else: return x\n",
    "    \n",
    "##########################################################    \n",
    "\n",
    "\n",
    "\n",
    "# ejecutando estas lineas se produce la limpieza de las celdas.\n",
    "df['REF_Flank1.locus_tag'] = df['REF_Flank1.locus_tag'].map(clean_type_1)\n",
    "df['REF_Flank1.locus_tag'] = df['REF_Flank1.locus_tag'].map(clean_type_2)\n",
    "df['REF_Flank1.locus_tag'] = df['REF_Flank1.locus_tag'].map(clean_type_3)\n",
    "df['REF_Flank1.locus_tag'] = df['REF_Flank1.locus_tag'].map(clean_type_4)\n",
    "df['REF_Flank1.locus_tag'] = df['REF_Flank1.locus_tag'].map(clean_type_5)\n",
    "df['REF_Flank1.locus_tag'] = df['REF_Flank1.locus_tag'].map(clean_type_6) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nota:\n",
    "# me gustaria agregar un contador para saber cuantas veces se corrigieron las celdas para cada uno de los casos,\n",
    "#pero todavia no me doy cuenta como hacerlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chequeamos que las celdas de 'REF_Flank1.locus_tag' hayan sido corregidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type_1\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 15:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_3\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 19:\n",
    "        print(i)\n",
    "    else: pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_4\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 24:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_5\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 10 or len(i) == 14:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_6\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 16:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado final de Data Cleaning de la columna 'REF_Flank1.locus_tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    print(f'\\n{i}')\n",
    "    \n",
    "# nota: si la data cleanning funciono correctamente cada linea de resultados deberia mostrar SOLO UN locus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df >>> {df.shape}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2 Data Cleanning 'REF_Flank2.locus_tag' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    print(f'\\n{i}')\n",
    "\n",
    "\"\"\"\n",
    "Los resultados que siguen a continuacion son casos especiales que deberiamos limpiar.\n",
    "IMPORTANTE: notar que cada tipo de dato que queremos limpiar tiene una len() particular, por\n",
    "lo tanto podemos elegir\n",
    "ese criterio para definir las funciones que vamos a usar para limpiar cada tipo de dato.\n",
    "\n",
    "type_1: 'BP1047 | BP1048'>>> for 7<x<16: split(' ') y quedarse con el el elemento [3] \n",
    "\n",
    "type_2: '-', '- | -', '- | - | -'\n",
    "\n",
    "type_3: 'BP0123 | BP0124 | -'\n",
    "\n",
    "type_4: 'BP3053 | BP3054 | BP3055'\n",
    "\n",
    "type_5: 'BP3508 | -' o 'BP0692 | - | -'\n",
    "\n",
    "type_6: 'BP2524 | BP2524A'  finalemente decidi que iba a dejar la letra A porque asi esta anotado en Tohama\n",
    "\n",
    "type_7: 'BP0101B' finalmente no lo considere como un typo de dato a limpiar porque en el genoma de Tohama esta anotado asi\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chequeamos la presencia de datos a corregir en las celdas de la columna \"'REF_Flank2.locus_tag'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# type_1\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 15:\n",
    "        print(i)\n",
    "    else: pass\n",
    "\n",
    "# x = 'BP1047 | BP1048'\n",
    "# x.split(' ')[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_2: '-', '- | -', '- | - | -'\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if i == '-' or i == '- | -' or i == '- | - | -':\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_3\n",
    "\n",
    "for i in df['REF_Flank1.locus_tag'].unique():\n",
    "    if len(i) == 19:\n",
    "        print(i)\n",
    "    else: pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_4\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 24:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_5: ej 'BP3508 | -' or 'BP0692 | - | -'\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 10 or len(i) == 14:\n",
    "        print(i)\n",
    "    else: pass\n",
    "\n",
    "# len('BP0692 | - | -')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_6: ej 'BP2524 | BP2524A'\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 16:\n",
    "        print(i)\n",
    "        \n",
    "    else: pass\n",
    "    \n",
    "# x = 'BP2029 | BP2029A'    \n",
    "\n",
    "# x.split(' ')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_7: ej 'BP1134 | - | - | -'\n",
    "# este caso no lo tenia del otro flanco\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 18:\n",
    "        print(i)\n",
    "        \n",
    "    else: pass\n",
    "# len('BP1134 | - | - | -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_8: ej 'BP1332A | BP1333 | BP1334'\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 25:\n",
    "        print(i)\n",
    "        \n",
    "    else: pass\n",
    "# len('BP1332A | BP1333 | BP1334')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Cleanning Functions-Flank2-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('ISs_Strains.csv')\n",
    "# df\n",
    "############################################\n",
    "\n",
    "# type_1: ej 'BP1047 | BP1048'>>> for 7<x<16: split(' ') y quedarse con el el elemento [2] \n",
    "def clean_type_1(x):    \n",
    "    if len(x) == 15:\n",
    "        return x.split(' ')[0]\n",
    "    else: \n",
    "        return x\n",
    "\n",
    "#######################################################\n",
    "def clean_type_2(x):\n",
    "    if x == '-' or x == '- | -' or x == '- | - | -':\n",
    "        return 'NaN'\n",
    "        \n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "#######################################################\n",
    "# type_3: ej 'BP0123 | BP0124 | -'\n",
    "def clean_type_3(x):\n",
    "    if len(x) == 19:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "        \n",
    "########################################################\n",
    "# type_4: ej 'BP3053 | BP3054 | BP3055'\n",
    "def clean_type_4(x):\n",
    "    if len(x) == 24:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "\n",
    "#######################################################\n",
    "# type_5: ej 'BP3508 | -'\n",
    "def clean_type_5(x):\n",
    "    if len(x) == 10 or len(x) == 14:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "\n",
    "#########################################################\n",
    "# type_6: ej 'BP2524 | BP2524A'\n",
    "def clean_type_6(x):\n",
    "    if len(x) == 16:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "    \n",
    "##########################################################  \n",
    "# type_7: ej 'BP1134 | - | - | -' \n",
    "def clean_type_7(x):\n",
    "    if len(x) == 18:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "\n",
    "##########################################################  \n",
    "# type_8: ej 'BP1332A | BP1333 | BP1334' \n",
    "def clean_type_8(x):\n",
    "    if len(x) == 25:\n",
    "        return x.split(' ')[0]\n",
    "    else: return x\n",
    "\n",
    "\n",
    "# ejecutando estas lineas se produce la limpieza de las celdas.\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_1)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_2)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_3)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_4)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_5)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_6)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_7)\n",
    "df['REF_Flank2.locus_tag'] = df['REF_Flank2.locus_tag'].map(clean_type_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chequeamos que las celdas de 'REF_Flank2.locus_tag' hayan sido corregidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type_1\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 15:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_3\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 19:\n",
    "        print(i)\n",
    "    else: pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_4\n",
    "\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 24:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_5\n",
    "\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 10 or len(i) == 14:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_6\n",
    "\n",
    "\n",
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    if len(i) == 16:\n",
    "        print(i)\n",
    "    else: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['REF_Flank2.locus_tag'].unique():\n",
    "    print(f'\\n{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df >>> {df.shape}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine REF.Product_Flank1 y REF.Product_Flank2\n",
    "Vamos a combinar las celdas de las columnas REF.Product_Flank1 y REF.Product_Flank2 de manera tal que al analizar los datos no tengamos que hacer dos analisis por separado y podamos enfocarnos solo en una columna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Insertion_site(REF)'] = df['REF_Flank1.locus_tag'] + '-' + df['REF_Flank2.locus_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['Insertion_site(REF)'].unique():\n",
    "    print(f'\\n{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df >>> {df.shape}')\n",
    "print(df[\"Insertion_site(REF)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "# Psrtial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'df >>> {df.shape}')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Save partial results as file.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ISs_Strains_clean.csv'\n",
    "df.to_csv(file, index=False)\n",
    "print(f'File \"{file}\" has been saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuacion vamos a agregar una columna que tenga en la que las celdas contengan el numero de cepas que se analizaron\n",
    "# el anio en que se analizo la cepa en cuestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('str_by_year.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "strains = []\n",
    "for key, value in b.items():\n",
    "    years.append(key)\n",
    "    strains.append(value)\n",
    "\n",
    "print(years)  \n",
    "print(strains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Year': years, 'Total_strains': strains}\n",
    "df_years = pd.DataFrame.from_dict(data=data)\n",
    "df_years\n",
    "\n",
    "# una vez creado esta DataFrame puedo mergear las dos DataFrames utilzando la columna Year para emparejarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'df_years >>> {df_years.shape}')\n",
    "print(f'df >>> {df.shape}')\n",
    "display(df_years.head(), df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con esta linea hacemos el MERGE!\n",
    "merge_df = pd.merge(df, df_years, left_on='Year', right_on='Year', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'merge_df >>> {merge_df.shape}')\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ISs_Strains_clean.csv'\n",
    "merge_df.to_csv(file, index=False)\n",
    "print(f'File \"{file}\" has been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
